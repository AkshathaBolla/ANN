{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AkshathaBolla/ANN/blob/main/TitanMining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQr5FARXNJdg"
      },
      "source": [
        "# Module I: Practice Exercise - 'Titan Mining Corp' (Refined Edition)\n",
        "\n",
        "### **Context: The Lunar Excavation**\n",
        "You are the Chief AI Geologist for 'Titan Mining Corp'. Automated drills are extracting core samples from Saturn's moon.\n",
        "Your sensors measure:\n",
        "1.  **Rock Density (g/cm³)**\n",
        "2.  **Radiation Level (mSv)**\n",
        "3.  **Magnetic Resonance (0-10)**\n",
        "\n",
        "You have **two separate goals**:\n",
        "* **Goal A (Discovery):** Predict if the sample contains **'Unobtanium' (1)** or just **'Dirt' (0)**. (Classification)\n",
        "* **Goal B (Appraisal):** Predict the **Market Value ($)** of the sample. (Regression)\n",
        "\n",
        "---\n",
        "**INSTRUCTIONS:**\n",
        "This is a **High-Friction Practice**.\n",
        "You must write the code to match the **Specific Architectures** provided below.\n",
        "Precision matters. If the blueprint asks for `Tanh`, do not use `ReLU`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "HjduAIgENJdj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8bccea0-50b2-4538-fd92-4a01b7fae007"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Titan Data Ready. X: torch.Size([200, 3]), y_discovery: torch.Size([200, 1]), y_value: torch.Size([200, 1])\n"
          ]
        }
      ],
      "source": [
        "# CELL 1: DATA GENERATION (Run this first)\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# 200 Core Samples\n",
        "N = 200\n",
        "# Features: Density (2-10), Radiation (0-50), Resonance (0-10)\n",
        "X = torch.rand(N, 3) * torch.tensor([8, 50, 10]) + torch.tensor([2, 0, 0])\n",
        "\n",
        "# Target A: Is Unobtanium? (Classification)\n",
        "# Rare: High Density AND High Resonance\n",
        "rare_score = (X[:, 0] * 2) + (X[:, 2] * 3) - (X[:, 1] * 0.5)\n",
        "y_discovery = (rare_score > 25).float().view(-1, 1)\n",
        "\n",
        "# Target B: Market Value (Regression)\n",
        "# Value = Density^2 + Resonance * 10\n",
        "y_value = (X[:, 0] ** 2) + (X[:, 2] * 10) + torch.randn(N) * 5\n",
        "y_value = y_value.view(-1, 1).float()\n",
        "\n",
        "print(f\"Titan Data Ready. X: {X.shape}, y_discovery: {y_discovery.shape}, y_value: {y_value.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1FtFxIxNJdk"
      },
      "source": [
        "## **Level 1: The Mineral Scanner (User Level)**\n",
        "**Your Task:** Build a model to classify samples as 'Unobtanium' or 'Dirt'.\n",
        "\n",
        "**Blueprint (Architecture):**\n",
        "1.  **Input Layer:** 3 Features\n",
        "2.  **Hidden Layer:** 32 Neurons, `Tanh` Activation\n",
        "3.  **Hidden Layer:** 16 Neurons, `Tanh` Activation\n",
        "4.  **Output Layer:** 1 Neuron, `Sigmoid` Activation\n",
        "\n",
        "**Training Specs:**\n",
        "* Loss: `BCELoss`\n",
        "* Optimizer: `Adam` (lr=0.01)\n",
        "* Epochs: 150"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "LpacBrtKNJdl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5e30c43-4229-4d4d-f995-de43fa357778"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9950000047683716\n"
          ]
        }
      ],
      "source": [
        "# LEVEL 1: WRITE YOUR CODE HERE\n",
        "\n",
        "# 1. Define Model Class 'MineralScanner'\n",
        "class MineralScanner(nn.Module):\n",
        "    # TODO: Implement __init__ with Linear(3,32) -> Tanh -> Linear(32,16) -> Tanh -> Linear(16,1) -> Sigmoid\n",
        "    def __init__(self):\n",
        "        super(MineralScanner, self).__init__()\n",
        "        self.layer1=nn.Linear(3,32)\n",
        "        self.layer2=nn.Tanh()\n",
        "        self.layer3=nn.Linear(32,16)\n",
        "        self.layer4=nn.Tanh()\n",
        "        self.layer5=nn.Linear(16,1)\n",
        "        self.layer6=nn.Sigmoid()\n",
        "    def forward(self,x):\n",
        "        x=self.layer1(x)\n",
        "        x=self.layer2(x)\n",
        "        x=self.layer3(x)\n",
        "        x=self.layer4(x)\n",
        "        x=self.layer5(x)\n",
        "        x=self.layer6(x)\n",
        "        return x\n",
        "model_scan = MineralScanner() # Initialize it\n",
        "\n",
        "# 2. Optimizer & Loss\n",
        "criterion=nn.BCELoss()\n",
        "optimizer=optim.Adam(model_scan.parameters(),lr=0.01)\n",
        "\n",
        "# 3. Training Loop (150 epochs)\n",
        "for epoch in range(150):\n",
        "  optimizer.zero_grad()\n",
        "  outputs=model_scan(X)\n",
        "  loss=criterion(outputs,y_discovery)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "# 4. Calculate Accuracy\n",
        "with torch.no_grad():\n",
        "  outputs=model_scan(X)\n",
        "  predicted=(outputs>0.5).float()\n",
        "  accuracy=(predicted==y_discovery).float().mean()\n",
        "  print(f\"Accuracy: {accuracy.item()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "LumiaYgpNJdl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e4092ab-6629-4aac-e919-5788b9854c21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Level 1 Passed: Architecture matches blueprint.\n"
          ]
        }
      ],
      "source": [
        "# TEST LEVEL 1\n",
        "try:\n",
        "    assert isinstance(model_scan, nn.Module), \"Model not initialized\"\n",
        "    # Check Architecture\n",
        "    modules = list(model_scan.children())\n",
        "    # Allow for Sequential or direct list\n",
        "    if isinstance(modules[0], nn.Sequential): modules = list(modules[0])\n",
        "\n",
        "    assert modules[0].out_features == 32, f\"Layer 1 must be 32. Found {modules[0].out_features}\"\n",
        "    assert isinstance(modules[1], nn.Tanh), \"Activation 1 must be Tanh\"\n",
        "    assert modules[2].out_features == 16, f\"Layer 2 must be 16. Found {modules[2].out_features}\"\n",
        "\n",
        "    print(\"✅ Level 1 Passed: Architecture matches blueprint.\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Level 1 Fail: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Bzd7beINJdl"
      },
      "source": [
        "## **Level 2: The Value Appraiser (Overfitting & DataLoaders)**\n",
        "**Your Task:** Predict the exact Market Value (Regression). You must deliberately overfit the model, then fix it using Dropout.\n",
        "\n",
        "**Step 2.1: The Pipeline**\n",
        "* Split `X` and `y_value` into **Train (80%)** and **Test (20%)**.\n",
        "* Create a `DataLoader` for training (Batch Size = 10). **This tests your ability to handle batches.**\n",
        "\n",
        "**Step 2.2: The Overfit Model (High Capacity)**\n",
        "* Create a model `model_overfit`:\n",
        "* Input(3) -> Linear(128) -> ReLU -> Linear(128) -> ReLU -> Output(1)\n",
        "* Train for 200 epochs. (You should see Training Loss go low, but Test Loss stay high).\n",
        "\n",
        "**Step 2.3: The Fix (Dropout)**\n",
        "* Create a model `model_regularized`:\n",
        "* Same architecture, but add `nn.Dropout(0.4)` after every ReLU.\n",
        "* This forces the model to learn robust features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "zH5T9yCTNJdl"
      },
      "outputs": [],
      "source": [
        "# LEVEL 2: WRITE YOUR CODE HERE\n",
        "\n",
        "# 1. Split Train/Test (Manual slicing or train_test_split)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,y_train,y_test=train_test_split(X,y_value,test_size=0.2,random_state=42)\n",
        "# 2. DataLoader (Batch Size 10)\n",
        "train_dataset=TensorDataset(X_train,y_train)\n",
        "test_dataset=TensorDataset(X_test,y_test)\n",
        "\n",
        "train_loader=DataLoader(train_dataset,batch_size=10,shuffle=True)\n",
        "test_loader=DataLoader(test_dataset,batch_size=10,shuffle=False)\n",
        "# 3. Define 'model_overfit' (No Dropout)\n",
        "class Modeloverfit(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.linear1=nn.Linear(3,128)\n",
        "    self.relu=nn.ReLU()\n",
        "    self.linear2=nn.Linear(128,128)\n",
        "    self.linear3=nn.Linear(128,1)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x=self.linear1(x)\n",
        "    x=self.relu(x)\n",
        "    x=self.linear2(x)\n",
        "    x=self.relu(x)\n",
        "    x=self.linear3(x)\n",
        "    return x\n",
        "model_overfit=Modeloverfit()\n",
        "loss_func=nn.BCEWithLogitsLoss()\n",
        "optimizer=optim.Adam(model_overfit.parameters(),lr=0.01)\n",
        "num_epochs=200\n",
        "batch_size=10\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    model_overfit.train()\n",
        "    for X_batch,y_batch in train_loader:\n",
        "      y_batch=y_batch.view(-1,1)\n",
        "      logits=model_overfit(X_batch)\n",
        "      loss=loss_func(logits,y_batch)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 4. Define 'model_regularized' (With Dropout 0.4)\n",
        "class Modelregularized(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.linear1=nn.Linear(3,128)\n",
        "    self.relu=nn.ReLU()\n",
        "    self.dropout=nn.Dropout(0.4)\n",
        "    self.linear2=nn.Linear(128,128)\n",
        "    self.linear3=nn.Linear(128,1)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x=self.linear1(x)\n",
        "    x=self.relu(x)\n",
        "    x=self.dropout(x)\n",
        "    x=self.linear2(x)\n",
        "    x=self.relu(x)\n",
        "    x=self.dropout(x)\n",
        "    x=self.linear3(x)\n",
        "    return x\n",
        "model_regularized=Modelregularized()\n",
        "loss_func=nn.BCEWithLogitsLoss()\n",
        "optimizer=optim.Adam(model_regularized.parameters(),lr=0.01)\n",
        "num_epochs=200\n",
        "batch_size=10\n",
        "for epoch in range(num_epochs):\n",
        "    model_regularized.train()\n",
        "    for X_batch,y_batch in train_loader:\n",
        "      y_batch=y_batch.view(-1,1)\n",
        "      logits=model_regularized(X_batch)\n",
        "      loss=loss_func(logits,y_batch)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "\n",
        "# 5. Optional: Train both to see the difference (Not graded, but recommended)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "yDKbcNekNJdm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d010b78-9cbd-45ec-8289-399d94e26719"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Level 2 Passed: Data Pipeline and Dropout correct.\n"
          ]
        }
      ],
      "source": [
        "# TEST LEVEL 2\n",
        "try:\n",
        "    # Check Dropout\n",
        "    has_dropout = any(isinstance(m, nn.Dropout) for m in model_regularized.modules())\n",
        "    assert has_dropout, \"Dropout layer not found in model_regularized\"\n",
        "\n",
        "    # Check Dropout Probability\n",
        "    for m in model_regularized.modules():\n",
        "        if isinstance(m, nn.Dropout):\n",
        "            assert m.p == 0.4, f\"Dropout must be 0.4, found {m.p}\"\n",
        "\n",
        "    # Check Dimensions\n",
        "    l1 = list(model_regularized.modules())[1] # First Linear\n",
        "    assert l1.out_features == 128, \"Hidden neurons must be 128\"\n",
        "\n",
        "    print(\"✅ Level 2 Passed: Data Pipeline and Dropout correct.\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Level 2 Fail: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUza9a8ZNJdm"
      },
      "source": [
        "## **Level 3: The Lab Tournament (Custom Combinations)**\n",
        "\n",
        "**Part 3.1: Manual Custom Loss**\n",
        "Implement `my_weighted_mse(pred, target)`.\n",
        "Sometimes, underestimating the value is worse than overestimating.\n",
        "$$ Loss = Mean( (pred - target)^2 * 1.5 ) $$\n",
        "(Multiply the standard squared error by 1.5).\n",
        "\n",
        "**Part 3.2: The Tournament**\n",
        "You must define the `experiments` list. Each experiment is a dictionary defining a specific combination of **Activation** and **Loss Function**.\n",
        "\n",
        "**The Contenders:**\n",
        "1.  **\"Standard\"**: Uses `nn.ReLU` and `nn.MSELoss`.\n",
        "2.  **\"Robust\"**: Uses `nn.LeakyReLU` (negative_slope=0.1) and `nn.L1Loss` (MAE).\n",
        "3.  **\"Custom\"**: Uses `nn.ELU` and **YOUR** `my_weighted_mse` function.\n",
        "\n",
        "**Task:** Fill in the `experiments` list with the correct objects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "BpRgVaLCNJdm"
      },
      "outputs": [],
      "source": [
        "# LEVEL 3.1: MANUAL CUSTOM LOSS\n",
        "def my_weighted_mse(logits, y_value):\n",
        "  loss=((logits-y_value)**2)*1.5\n",
        "    # TODO: Return (pred - target)^2 * 1.5\n",
        "  return loss\n",
        "\n",
        "# LEVEL 3.2: TOURNAMENT CONFIG\n",
        "# TODO: Fill in the 'None' values with actual Class Instances or Functions\n",
        "experiments = [\n",
        "    {\"name\": \"Standard\", \"act\": nn.ReLU(), \"loss\": nn.MSELoss()}, # ReLU, MSELoss\n",
        "    {\"name\": \"Robust\",   \"act\": nn.LeakyReLU(negative_slope=0.1), \"loss\": nn.L1Loss()}, # LeakyReLU(0.1), L1Loss\n",
        "    {\"name\": \"Custom\",   \"act\": nn.ELU(), \"loss\": my_weighted_mse}  # ELU, my_weighted_mse\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "AY1sxnoFNJdm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c105f267-8917-4300-d492-5197b9ee4984"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Level 3 Passed: Manual Loss and Tournament Configs are correct.\n"
          ]
        }
      ],
      "source": [
        "# TEST LEVEL 3\n",
        "try:\n",
        "    # Test Manual Loss\n",
        "    p = torch.tensor([2.0]); t = torch.tensor([4.0])\n",
        "    # (2-4)^2 * 1.5 = 4 * 1.5 = 6.0\n",
        "    assert my_weighted_mse(p, t).item() == 6.0, \"Math Fail: Weighted MSE calculation incorrect\"\n",
        "\n",
        "    # Test Configs\n",
        "    assert isinstance(experiments[0]['loss'], nn.MSELoss), \"Standard must use MSELoss\"\n",
        "    assert experiments[1]['act'].negative_slope == 0.1, \"Robust must use LeakyReLU with 0.1 slope\"\n",
        "    assert not isinstance(experiments[2]['loss'], type), \"Custom must use your function, not a class\"\n",
        "\n",
        "    print(\"✅ Level 3 Passed: Manual Loss and Tournament Configs are correct.\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Level 3 Fail: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Dtso8DyNJdm"
      },
      "source": [
        "## **Level 4: The Core Engineer (Expert Level)**\n",
        "**Your Task:** The `optim.Adam` and `loss.backward()` buttons are broken.\n",
        "Perform **One Step of Gradient Descent** manually.\n",
        "\n",
        "1.  Model: $y = w \\cdot x$ (No bias for simplicity).\n",
        "2.  Input $x=2.0$, Target $y=10.0$.\n",
        "3.  Initial Weight $w=3.0$.\n",
        "4.  Loss Function: Squared Error $L = (wx - y)^2$.\n",
        "5.  **Calculate Gradient:** $\\frac{dL}{dw} = 2(wx - y) \\cdot x$.\n",
        "6.  **Update Weight:** $w_{new} = w - lr \\cdot gradient$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "aqgV07S0NJdn"
      },
      "outputs": [],
      "source": [
        "# LEVEL 4: THE REPAIR JOB\n",
        "lr = 0.1\n",
        "x = 2.0\n",
        "y = 10.0\n",
        "w = 3.0\n",
        "\n",
        "# TODO: Calculate prediction\n",
        "pred = w*x\n",
        "\n",
        "# TODO: Calculate squared error\n",
        "loss = pred-y\n",
        "\n",
        "# TODO: Calculate gradient manually (Derived from Chain Rule)\n",
        "gradient = 2*loss*x\n",
        "\n",
        "# TODO: Update w\n",
        "w_new = w-(lr*gradient)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "2EaXz96BNJdn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "894116aa-747d-47b6-92b4-b498b5db7e3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Level 4 Passed: Manual Backprop successful!\n"
          ]
        }
      ],
      "source": [
        "# TEST LEVEL 4\n",
        "try:\n",
        "    # Pred = 3*2 = 6\n",
        "    # Error = (6-10) = -4\n",
        "    # Grad = 2 * (-4) * 2 = -16\n",
        "    # w_new = 3.0 - (0.1 * -16) = 3.0 + 1.6 = 4.6\n",
        "\n",
        "    assert abs(w_new - 4.6) < 0.01, f\"Math Fail: New weight should be 4.6, got {w_new}\"\n",
        "    print(\"✅ Level 4 Passed: Manual Backprop successful!\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Level 4 Fail: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EFl3bEi1CYPs"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}